# export ANSIBLE_HOST_KEY_CHECKING=False
# apt update
- name: Run the equivalent of "apt-get update"
  apt:
     update_cache: yes
  become: true

- name: Java install
  shell: "apt install openjdk-8-jdk -y"
  become: true

- name: Scala install
  shell: "apt-get install scala -y"
  become: true

#- name: Download Spark
#  ansible.builtin.get_url:
#    url: https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
#    dest: /home/student

#- name: Unzip Spark
#  shell: "tar xvf spark-3.1.1-bin-hadoop3.2.tgz"
#  become: true

#- name: Unarchive a file that is already on the remote machine
#  unarchive:
#    src: /tmp/foo.zip
#    dest: /usr/local/bin
#    remote_src: yes

#- name: Create home dir for Spark
#  shell: "mv spark-3.1.1-bin-hadoop3.2 /usr/local/spark"
#  become: true

#- name: Update bashrc
#  lineinfile:
#    dest: /home/root/.bashrc
#    regexp: '^#fi'
#    line: "PATH=$PATH:/usr/local/spark/bin"

- name: append environment variables to the end of .bashrc file
  blockinfile:
    dest: "~/.bashrc"
    insertafter: EOF
    block: |
      # environment variable exports
      export PATH="$PATH:/usr/local/spark/bin";

- name: Save bashrc
  shell: source ~/.bashrc
  become: true

