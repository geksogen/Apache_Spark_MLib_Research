# export ANSIBLE_HOST_KEY_CHECKING=False
- name: Run the equivalent of "apt-get update"
  apt:
     update_cache: yes
  become: true

- name: Install pip3
  apt:
    name: python3-pip
    state: present
  become: true

- name: Install python package
  ansible.builtin.pip:
    name:
      - pandas
      - s3fs

- name: Add IP to hosts file
  blockinfile:
    dest: "/etc/hosts"
    insertafter: EOF
    block: |
      # Spark cluster, Public IP
      62.84.125.93   sp-master
      89.169.159.93  sp-slave1
      89.169.148.115 sp-slave2
  become: true

- name: Java install
  shell: "apt install openjdk-8-jdk -y"
  become: true

- name: Scala install
  shell: "apt-get install scala -y"
  become: true

- name: Create sp-user
  user:
    name: sp-user
    password: "{{ '123' | password_hash('sha512') }}"
    shell: /bin/bash
    state: present
  become: true

- name: Add user to sudo group
  user:
    name: username
    groups: sudo
    append: yes
  become: true

- name: Allow passwordless sudo for the user
  blockinfile:
    dest: "/etc/sudoers"
    insertafter: EOF
    block: |
      # environment variable exports
      sp-user ALL=(ALL:ALL) ALL
      sp-user ALL=(ALL:ALL) NOPASSWD: ALL
  become: true

- name: Download Spark
  ansible.builtin.get_url:
    url: https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
    dest: /home/student

- name: Unzip Spark
  shell: "tar xvf spark-3.1.1-bin-hadoop3.2.tgz"
  become: true

- name: Create home dir for Spark
  shell: "mv spark-3.1.1-bin-hadoop3.2 /home/sp-user/spark"
  become: true

- name: append environment variables to the end of .bashrc file
  blockinfile:
    dest: "/home/sp-user/.bashrc"
    insertafter: EOF
    block: |
      # environment variable exports
      export PATH="$PATH:/home/sp-user/spark/bin";
  become: true

- name: Save bashrc
  shell: source /home/sp-user/.bashrc
  args:
    executable: /bin/bash

- name: Add slave host
  copy:
      content: |
        sp-slave1
        sp-slave2
      dest: /home/sp-user/spark/conf/slaves

- name: Set chmod 777 to Spark work dir
  file:
    path: /home/sp-user/spark
    mode: 0777
    recurse: yes

- hosts: Copy env config
  tasks:
    - copy:
        src: /home/sp-user/spark/conf/spark-env.sh.template
        dest: /home/sp-user/spark/conf/spark-env.sh

- name: Set env config
  blockinfile:
    dest: "/home/sp-user/spark/conf/spark-env.sh"
    insertafter: EOF
    block: |
      # environment variable exports
      export SPARK_MASTER_HOST=<IP internal>
      export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
  become: true



#- name: Copy spark-env # закинуть локальный файл
#  copy:
#    content: |
#      sp-slave1
#      sp-slave2
#    dest: /home/sp-user/spark/conf/slaves


#- name: Copy template env config
#  ansible.builtin.copy:
#    src: /home/sp-user/spark/conf/spark-env.sh.template
#    dest: /home/sp-user/spark/conf/spark-env.sh
#  become: true
